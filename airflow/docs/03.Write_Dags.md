# Write Dags


## 1. First sample dag

Let's write our first Dag definition file: `sample_dag1.py`

```python
import textwrap
from datetime import datetime, timedelta
from airflow.models.dag import DAG
from airflow.operators.bash import BashOperator

with DAG(
    "sample_dag1",
    default_args={
        "depends_on_past": False,
        "email": ["pengfei.liu@casd.eu"],
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    description="A simple tutorial DAG",
    schedule=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=["example"],
) as dag:
    t1 = BashOperator(
        task_id="task_1",
        bash_command="echo 'task 1'",
    )

    t2 = BashOperator(
        task_id="task_2",
        depends_on_past=False,
        bash_command="echo 'task 2'",
        retries=3,
    )
    templated_command = textwrap.dedent(
        """
    {% for i in range(5) %}
        echo "task 3"
    {% endfor %}
    """
    )

    t3 = BashOperator(
        task_id="task_3",
        depends_on_past=False,
        bash_command=templated_command,
    )

    t1 >> t2 >> t3

```

> Python script is really just a configuration file specifying the DAG’s structure as code

We can divide this code into parts:
- import dependencies
- instantiate a Dag
- define tasks
- set up task dependencies

### 1.1 import dependencies

The import only works on python packages. And the import rules is the same for all python scripts

```python
import textwrap
from datetime import datetime, timedelta

# The DAG object; we'll need this to instantiate a DAG
from airflow.models.dag import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
```

### 1.2 instantiate a dag

To instantiate a dag, we use the python `with ... as:` to create a variable `dag`. A dag has below argument:
- dag_id: in the below code the value is sample_dag1
- default_args: We can put any key, value pair here to explicitly pass a set of arguments to each task’s constructor
- description: what this dag will do
- schedule: how often the dag will be triggered
- start_date:
- catchup: if true, miss the scheduled run will be run if possible
- tags: assign a tag to the dag
```python
with DAG(
    "sample_dag1",
    default_args={
        "depends_on_past": False,
        "email": ["pengfei.liu@casd.eu"],
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    description="A simple tutorial DAG",
    schedule=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=["example"],
) as dag:
```

### 1.3 Define tasks

Here we define 3 tasks with `BashOperator`, the task `t3` calls a templated command, airflow uses `jinja` as template 
framework. For example `{% for i in range(5) %}` is a jinja code fragment do the for loop.

```python
t1 = BashOperator(
        task_id="task_1",
        bash_command="echo 'task 1'",
    )

    t2 = BashOperator(
        task_id="task_2",
        depends_on_past=False,
        bash_command="echo 'task 2'",
        retries=3,
    )
    templated_command = textwrap.dedent(
        """
    {% for i in range(5) %}
        echo "task 3"
    {% endfor %}
    """
    )

    t3 = BashOperator(
        task_id="task_3",
        depends_on_past=False,
        bash_command=templated_command,
    )
```

### 1.4 set up task dependencies

As a dag is directed, we need to define orders for all tasks. In the below code, we define, t1 runs first, if ok, then
t2, if t2 ok then t3.

```python
t1 >> t2 >> t3
```

We have more python way to define the task dependencies. Below is an example

```python
t1.set_downstream(t2)

# This means that t2 will depend on t1 running successfully to run.
# It is equivalent to:
t2.set_upstream(t1)

# The bit shift operator can also be used to chain operations:
t1 >> t2

# And the upstream dependency with the
# bit shift operator:
t2 << t1

# Chaining multiple dependencies becomes
# concise with the bit shift operator:
t1 >> t2 >> t3

# A list of tasks can also be set as
# dependencies. These operations
# all have the same effect:
t1.set_downstream([t2, t3])
t1 >> [t2, t3]
[t2, t3] << t1
```

## 2. Deploy the dag in the airflow server

In your `AIRFLOW_HOME`, you can find a config file called `aiflow.cfg`. You should find the below line

```text
[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
#
# Variable: AIRFLOW__CORE__DAGS_FOLDER
#
dags_folder = /home/pengfei/opt/airflow/dags

```

you need to put the `sample_dag1.py` file in the `dags_folder`. If your scheduler is active, the next refresh will load
the new dag into the web server.


## 3. Testing the dag

We can test a dag or a task(inside a dag) by using the below command

```shell
# runs task instances locally, outputs their log to stdout (on screen), does not bother with dependencies, 
# and does not communicate state (running, success, failed, …) to the database. 
# It simply allows testing a single task instance.
airflow tasks test <dag_id> <task_id> <start_date>

# It performs a single DAG run of the given DAG id. While it does take task dependencies into account, 
# no state is registered in the database. It is convenient for locally testing a full run of your DAG, given that 
# e.g. if one of your tasks expects data at some location, it is available
airflow dags test <dag_id> <start_date>

# for example with the above dag loaded, you can run the below test
# test task_1 of the sample_dag1
airflow tasks test sample_dag1 task_1

# test the dag sample_dag1
airflow dags test sample_dag1
```


## 4. Update and delete dag

You just need to modify or delete the `sample_dag1.py` file in the `dags_folder`. The `airflow scheduler` will update/
delete automatically.